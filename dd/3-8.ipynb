{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path \n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found: 1024\n",
      "Number of labels found: 1024\n",
      "Number of unique characters: 23\n",
      "Characters present: ['2', '3', '4', '5', '6', '7', '8', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'm', 'n', 'p', 's', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Path to the data directory\n",
    "data_dir = Path(\"./samples\")\n",
    "\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "characters = sorted(characters)\n",
    "print(f\"Number of images found: {len(images)}\")\n",
    "print(f\"Number of labels found: {len(labels)}\")\n",
    "print(f\"Number of unique characters: {len(characters)}\")\n",
    "print(f\"Characters present: {characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Batch size for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "# Desired image dimensions\n",
    "img_width = 300\n",
    "img_height = 50\n",
    "\n",
    "# Factor by which the image is going to be downsampled by the convolutional blocks.\n",
    "# We will be using two convolution blocks and each block will have a pooling layer\n",
    "# which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "max_length = max([len(label) for label in labels])\n",
    "print(max_length)\n",
    "for label in labels:\n",
    "    if(len(label)<6):\n",
    "        print(label)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pouria/mambaforge/envs/ten/lib/python3.11/site-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]] \n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": img, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting data into training and validation sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_Train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_Train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(images, labels, train_size, shuffle)\u001b[0m\n\u001b[1;32m     16\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(size)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Get the size of training samples\u001b[39;00m\n\u001b[1;32m     20\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(size \u001b[38;5;241m*\u001b[39m train_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "print(f\"X_Train shape: {X_train.shape}\")\n",
    "print(f\"y_Train shape: {y_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    " \n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        \n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        \n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # At test, just return the computed predictions\n",
    "        return y_pred\n",
    "    \n",
    "def build_model():\n",
    "    # Inputs the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    \n",
    "    # First conv block\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv1\")(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "    \n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv2\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "    \n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller.\n",
    "    # The number of filters in the last layer is 64. \n",
    "    # Reshape accordingly before passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    \n",
    "    # Output layer\n",
    "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "    \n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "    \n",
    "    # Define the model \n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the model\n",
    "\n",
    "model = build_model()\n",
    "#model.save('/home/pouria/Desktop/dd/')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "#model = load_model('/home/pouria/Desktop/dd/')\n",
    "# Add early stopping\n",
    "\n",
    "#early_stopping = keras.callbacks.EarlyStopping(\n",
    " #monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    "#)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#history = model.fit(\n",
    "#    train_dataset, \n",
    "#    validation_data=validation_dataset,\n",
    "#    epochs=epochs,\n",
    "#    callbacks=[early_stopping]\n",
    "#)\n",
    "#model.save('/Users/pouriajangjoo/Desktop/tensorflow_macos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction model by extracting layers till the output layer\n",
    "model = keras.models.load_model('/Users/pouria/Desktop/osx ')\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#predict the result\n",
    "prediction_model.summary()\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, : max_length]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "# Let's check results on some validation samples\n",
    "for batch in validation_dataset.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "    \n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    \n",
    "    orig_texts = []\n",
    "    for label in batch_labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "        \n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n",
    "    for i in range(len(pred_texts)):\n",
    "        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n",
    "        img = img.T\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")  \n",
    "      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from datetime import datetime,timedelta\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from multiprocessing import Process\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import base64\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "import requests\n",
    "import time\n",
    "def saveImage(html,name):\n",
    "    html = str(html)\n",
    "    list = html.split('data:image/jpg;base64,')\n",
    "    list2 = list[1].split('\\'')\n",
    "    data = list2[0]\n",
    "    im = Image.open(BytesIO(base64.b64decode(data)))\n",
    "    im.save(name+'.png', 'PNG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getCaptcha():\n",
    "    test_images = list(map(str, list(Path(\"./test/\").glob(\"*.png\"))))\n",
    "    test_labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in test_images]\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "    test_dataset = (\n",
    "        test_dataset.map(\n",
    "            encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    for batch in test_dataset.take(1):\n",
    "        pred_texts = decode_batch_predictions(prediction_model.predict(batch[\"image\"]))\n",
    "    return str(pred_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tarikhe ye rooz qabl\n",
    "kar=\"https://service2.diplo.de/rktermin/extern/appointment_showDay.do?locationCode=amma&realmId=16&categoryId=44&dateStr=19.09.2021\"\n",
    "urlirani = \"https://service2.diplo.de/rktermin/extern/appointment_showDay.do?locationCode=eriw&realmId=351&categoryId=569&dateStr=30.09.2021\"\n",
    "url = \"https://service2.diplo.de/rktermin/extern/appointment_showDay.do?locationCode=eriw&realmId=351&categoryId=1513&dateStr=18.09.2021\"\n",
    "#date = \"06.04.2021\"\n",
    "x = datetime(2021,9,17,0,29,0)\n",
    "def open(sec):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--window-size=800,600\");\n",
    "    options.page_load_strategy = 'none'\n",
    "    #options.add_argument(\"--disable-javascript\")\n",
    "    #prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    #options.add_experimental_option(\"prefs\", prefs)\n",
    "    #options.add_extension('/Users/pouriajangjoo/Desktop/extension_0_1_0_0.crx')\n",
    "    browser = webdriver.Chrome(options=options,executable_path='/Users/pouriajangjoo/Desktop/osx/dd/chromedriver')\n",
    "    #browser=webdriver.Firefox(executable_path='/Users/pouriajangjoo/Desktop/osx/dd/geckodriver')\n",
    "    browser.get(kar)\n",
    "    time.sleep(3)\n",
    "    saveImage(str(browser.page_source), 'test/mmmmmm')\n",
    "    browser.find_element(By.ID,'appointment_captcha_day_captchaText').send_keys(getCaptcha())\n",
    "    browser.execute_script(\"\"\"document.querySelector(\"input[name='action:appointment_showDay']\").click();\"\"\")\n",
    "    while(x > datetime.now()):\n",
    "        pass \n",
    "    browser.get('https://service2.diplo.de/rktermin/extern/appointment_showForm.do?locationCode=amma&realmId=16&categoryId=44&dateStr=23.09.2021&openingPeriodId=61711')\n",
    "    start = datetime.now()\n",
    "    print(datetime.now())\n",
    "    saveImage(str(browser.page_source), 'test/mmmmmm')\n",
    "    browser.find_element(By.ID,'appointment_newAppointmentForm_captchaText').send_keys(getCaptcha())\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_lastname\").value=\"keshvari\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_firstname\").value=\"arash\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_email\").value=\"@gmail.com\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_emailrepeat\").value=\"@gmail.com\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_fields_0__content\").value=\"F49698846\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_fields_1__content\").value=\"003749308457\";\"\"\")\n",
    "     #shomare_telepono_avaz_kon\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_fields_2__content\").value=\"Studium / studies\";\"\"\")\n",
    "    browser.execute_script(\"\"\"document.getElementById(\"appointment_newAppointmentForm_fields_3__content\").value=\"Iran\";\"\"\")\n",
    "    element=browser.find_element(By.ID,\"appointment_newAppointmentForm_appointment_addAppointment\")\n",
    "    javaScript = \"document.getElementById('appointment_newAppointmentForm_appointment_addAppointment').click();\"\n",
    "    end = start + timedelta(seconds = sec)\n",
    "    while(end > datetime.now()):\n",
    "         pass\n",
    "    browser.execute_script(javaScript);\n",
    "    print(datetime.now())\n",
    "sec = 3.94\n",
    "t1 = threading.Thread(target=open , args=(sec,))\n",
    "t1.start()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
